# Projeto de Disciplina de Processamento de Linguagem Natural com Python [25E2_2]

<img style='width:350px' src='./assets/logo_infnetv2.png' alt='Infnet logo' align='left'>
<br>
Projeto desenvolvido na disciplina de Processamento de Linguagem Natural com Python para aplicar t√©cnicas de an√°lise de texto, extra√ß√£o de t√≥picos e modelagem de linguagem, utilizando ferramentas modernas como spaCy, NLTK e LDA.

<br>
<br>
<br>

## √çndice

- <a href='#contexto'>1. Contexto</a>
- <a href='#tecnologias'>2. Tecnologias</a>
- <a href='#configura√ß√£o-inicial'>3. Configura√ß√£o inicial</a>
- <a href='#fluxograma-do-projeto'>4. Fluxograma do projeto
    - <a href='#parte-01-pegar-os-dados'>4.1. Pegar os dados
    - <a href='#parte-02-pr√©-processamento-dos-dados'>4.2. Pr√©-processamento dos dados
    - <a href='#parte-03-gera√ß√£o-de-documentos-spacy'>4.3. Gera√ß√£o de documentos `spaCy`
    - <a href='#parte-04-lematiza√ß√£o'>4.4. Lematiza√ß√£o
    - <a href='#parte-05-reconhecimento-de-entidades-nomeadas-ner'>4.5. Reconhecimento de Entidades Nomeadas (NER)
    - <a href='#parte-06-bag-of-words'>4.6. Bag of Words
    - <a href='#parte-07-extra√ß√£o-de-t√≥picos'>4.7. Extra√ß√£o de T√≥picos
    - <a href='#parte-08-visualiza√ß√£o-dos-dados'>4.8. Visualiza√ß√£o dos dados
- <a href='#conclus√µes-gerais'>5. Conclus√µes gerais</a>
- <a href='#sobre-mim'>6. Sobre mim</a> 


## Contexto

Este projeto foi desenvolvido no contexto da disciplina de Processamento de Linguagem Natural com Python, e visa aplicar t√©cnicas avan√ßadas para an√°lise de textos, extra√ß√£o de t√≥picos e modelagem sem√¢ntica. Utilizando um conjunto de not√≠cias da se√ß√£o "Mercado" da Folha de S. Paulo (2016), exploramos m√©todos como TF-IDF, LDA e reconhecimento de entidades nomeadas, permitindo uma interpreta√ß√£o estruturada dos dados. Al√©m disso, buscamos aprimorar a compreens√£o sobre vetoriza√ß√£o de textos, aprendizado n√£o supervisionado e visualiza√ß√£o de informa√ß√µes, proporcionando uma abordagem pr√°tica e aprofundada na √°rea de Processamento de Linguagem Natural.


## Tecnologias

<img style='width:30px; vertical-align: middle; margin-right: 10px' src='https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/anaconda/anaconda-original.svg' alt='Anaconda logo'> Anaconda v. 23.7.4

<img style='width:30px; vertical-align: middle; margin-right: 10px;' src='https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/jupyter/jupyter-original-wordmark.svg' alt='Jupyter logo'> Jupyter Notebook v. 5.7.2

<img style='width:30px; vertical-align: middle; margin-right: 10px' src='https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/python/python-original.svg' alt='Python logo'> Python v. 3.11.11

Principais bibliotecas:

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/kaggle/kaggle-original.svg" alt='kaggle_logo'> Kaggle 1.7.4.5

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/matplotlib/matplotlib-original.svg" alt='matplotlib_logo'> Matplotlib 3.7.5

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="./assets/general_lib.png" alt='nltk_logo'> NLTK 3.9.1

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/numpy/numpy-original.svg" alt='numpy_logo'> Numpy 1.26.4

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/pandas/pandas-original.svg" alt='pandas_logo'> Pandas 2.2.3


- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/pypi/pypi-original.svg"/> Pypi pip-tools
          

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src='https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/scikitlearn/scikitlearn-original.svg' alt='scikit-learn_logo'> Scikit-learn 1.4.2

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://seaborn.pydata.org/_images/logo-mark-lightbg.svg" alt='seaborn_logo'> Seaborn 0.13.2

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://upload.wikimedia.org/wikipedia/commons/8/88/SpaCy_logo.svg" alt='spacy_logo'> spaCy 3.8.4

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="https://raw.githubusercontent.com/tqdm/img/47dd765d1c88d70f65a3d2ce08430ffb175a9d53/logo.svg" alt='tqdm_logo'> tqdm 4.67.1

- <img style='width:30px; vertical-align: middle; margin-right: 10px' src="./assets/general_lib.png" alt='wordcloud_logo'> Wordcloud 1.9.4


## Configura√ß√£o inicial

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

O projeto foi realizado no ambiente virtual 'üåê env_25E2_2', instalado pelo Anconda.

Para garantir a instala√ß√£o eficiente das depend√™ncias, utilizamos o pip-tools, uma ferramenta que permite gerenciar pacotes de forma controlada e reproduz√≠vel.


## Fluxograma do projeto

O projeto segue o seguinte fluxograma (vamos mostrar por partes):

### Parte 01. Pegar os dados

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

Os dados utilizados neste projeto foram obtidos a partir do Kaggle. O download foi realizado por meio de sua API, que permitiu acesso direto ao conjunto de not√≠cias extra√≠das da se√ß√£o "Mercado" da Folha de S. Paulo.

Para isso, o comando `kaggle.api.dataset_download_files()` foi utilizado, garantindo que os arquivos fossem salvos no diret√≥rio especificado (`"../data/01-raw/"`) e extra√≠dos automaticamente (`unzip=True`). Esse processo facilita a manipula√ß√£o dos dados no restante do pipeline de an√°lise.

### Parte 02. Pr√©-processamento dos dados

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

O pr√©-processamento dos textos √© visa garantir a qualidade da an√°lise de t√≥picos. Nesse processo, foi aplicado diversas t√©cnicas de limpeza e transforma√ß√£o dos dados, garantindo que as informa√ß√µes estejam organizadas e prontas para os modelos de aprendizado de m√°quina. Dentre elas est√£o:

- **Remo√ß√£o de caracteres especiais e pontua√ß√£o**: onde s√≠mbolos irrelevantes s√£o eliminados para evitar ru√≠dos no modelo;

- **Normaliza√ß√£o de texto**: convertendo tudo para letras min√∫sculas para padronizar a an√°lise;

- **Tokeniza√ß√£o**: o texto √© separado em tokens individuais, para segmentar as palavras;

- **Remo√ß√£o de stopwords e filtragem de palavrs indesejadas**: exclu√≠mos palavras muito frequentes e sem relev√¢ncia sem√¢ntica, como 'o', '√©' e 'de'. Al√©m da remo√ß√£o de palavras sem muito valor sem√™ntico para a an√°lise, como 'chegar', 'haver', 'ficar';

- **Stemming e Lemmatiza√ß√£o**: redu√ß√£o das palavras √† sua raiz ou forma base, preservando seu significado.

Al√©m disso, foram criadas as seguinte colunas no dataset para realizar a an√°lise dos dados:

- `nltk` ‚Äì Cont√©m a vers√£o tokenizada do texto, utilizando nltk.word_tokenize();

- `tokenizer` ‚Äì Representa as palavras j√° filtradas, sem stopwords e caracteres especiais;

- `stemmer` ‚Äì Apresenta as palavras reduzidas √† sua forma radical, aplicando o algoritmo RSLP do NLTK.

Essas transforma√ß√µes ajudam a reduzir a dimensionalidade do texto e melhorar sua estrutura, facilitando etapas posteriores como extra√ß√£o de t√≥picos e vetoriza√ß√£o.


### Parte 03. Gera√ß√£o de documentos `spaCy`

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

A utiliza√ß√£o do `spaCy` permite processar textos de forma eficiente e estruturada, fornecendo uma an√°lise lingu√≠stica robusta para cada documento. Nesta etapa, cada texto do dataset foi convertido em um objeto spaCy, o que facilitou opera√ß√µes como tokeniza√ß√£o, lematiza√ß√£o, reconhecimento de entidades nomeadas (NER) e diversas an√°lises lingu√≠sticas.

Para realizar essa convers√£o, utilizou-se o modelo `pt_core_news_lg`, um dos mais avan√ßados para a l√≠ngua portuguesa. Esse modelo foi carregado e seu processamento foi aplicado em cada texto do dataset, gerando uma nova coluna chamada `spacy_doc`. Essa coluna cont√©m os objetos `spaCy` correspondentes aos textos, permitindo que as an√°lises subsequentes sejam feitas diretamente sobre essas representa√ß√µes lingu√≠sticas.

A aplica√ß√£o do modelo foi realizada por meio da fun√ß√£o `progress_apply(nlp)`, garantindo efici√™ncia no processamento de um grande volume de dados. Embora essa opera√ß√£o possa levar alguns minutos para ser conclu√≠da, ela √© essencial para preparar os textos para an√°lises posteriores, como extra√ß√£o de t√≥picos, reconhecimento de entidades nomeadas e vetoriza√ß√£o dos textos.

### Parte 04. Lematiza√ß√£o

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

A `lematiza√ß√£o` √© uma t√©cnica de processamento de linguagem natural que transforma palavras (ex: 'trabalhou') em suas formas base (ex: 'trabalhar'), garantindo que varia√ß√µes morfol√≥gicas sejam reduzidas sem perder o significado do texto. Ao contr√°rio do `stemming`, que apenas corta sufixos, a `lematiza√ß√£o` usa conhecimento lingu√≠stico para encontrar a forma mais apropriada de cada termo, tornando a an√°lise mais precisa.

Nesta etapa, o `spaCy` foi utilizado, j√° que fornece suporte nativo √† lematiza√ß√£o em portugu√™s, algo que o `NLTK` n√£o possui. Para garantir um pr√©-processamento eficiente, criamos uma lista combinada de stopwords provenientes do `NLTK` e do `spaCy`, removendo palavras muito frequentes que n√£o agregam significado √† an√°lise de t√≥picos.

Al√©m disso, foi implemetado um filtro que exclui tokens indesejados, garantindo que os lemmas extra√≠dos sejam relevantes para o estudo. A fun√ß√£o `lemma()` aplica esse processamento a cada documento no dataset, gerando a coluna `spacy_lemma`, que cont√©m os tokens lematizados de cada texto. 

Essa transforma√ß√£o melhora a qualidade dos dados para extra√ß√£o de t√≥picos e an√°lise sem√¢ntica, reduzindo ru√≠dos e tornando os vetores textuais mais representativos.

### Parte 05. Reconhecimento de Entidades Nomeadas (NER)

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

O Reconhecimento de Entidades Nomeadas (NER) √© uma t√©cnica essencial no Processamento de Linguagem Natural, utilizada para identificar e categorizar elementos importantes dentro de um texto, como pessoas, organiza√ß√µes, locais e datas. Essa abordagem permite estruturar informa√ß√µes n√£o estruturadas, facilitando a an√°lise de rela√ß√µes entre entidades.

Nesta etapa, o modelo do `spaCy` √© utilizado para extrair organiza√ß√µes mencionadas nos textos. Para isso, a fun√ß√£o `NER()` percorre cada documento e identifica entidades classificadas como `"ORG"`, armazenando-as na coluna `spacy_ner` do dataset. Esse processo possibilita a visualiza√ß√£o das empresas, institui√ß√µes ou marcas mais citadas nos textos, auxiliando na identifica√ß√£o de padr√µes e tend√™ncias do mercado.

### Parte 06. Bag of Words

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

A t√©cnica de Bag of Words (BoW) √© um m√©todo utilizado para representar textos numericamente, transformando-os em vetores baseados na frequ√™ncia de ocorr√™ncia de palavras. Essa abordagem ignora a estrutura gramatical e a ordem das palavras, focando apenas na presen√ßa e frequ√™ncia dos termos, o que facilita an√°lises estat√≠sticas e aprendizado de m√°quina.

Nesta etapa, o `TF-IDF` (Term Frequency-Inverse Document Frequency) √© utilizado para atribuir pesos aos termos, destacando aqueles que s√£o mais representativos no contexto dos documentos. Para isso, foi criada a coluna `tfidf` no dataframe `news_2016`, utilizando a coluna `spacy_lemma` como base para o c√°lculo dos vetores `TF-IDF`.

O n√∫mero m√°ximo de caracter√≠sticas consideradas no modelo √© 5000, e um token precisa ter aparecido pelo menos 10 vezes (`min_df`) nos documentos para ser inclu√≠do na matriz. Para garantir que os dados estejam organizados, uma classe `Vectorizer` foi implementada para pr√©-processar os textos e transformar listas de tokens em vetores, utilizando a biblioteca scikit-learn (`TfidfVectorizer`).

A fun√ß√£o `tokens2tfidf()` converte os tokens lematizados em vetores TF-IDF, permitindo que sejam armazenados na coluna `tfidf` do dataset. Essa representa√ß√£o melhora a an√°lise de t√≥picos e possibilita identificar padr√µes de relev√¢ncia sem√¢ntica no conjunto de dados.

### Parte 07. Extra√ß√£o de T√≥picos

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>


A extra√ß√£o de t√≥picos √© uma t√©cnica que permite identificar padr√µes sem√¢nticos e agrupar documentos por temas latentes. Para isso, o Latent Dirichlet Allocation (LDA) √© utilizado, um modelo probabil√≠stico que associa palavras a t√≥picos, gerando uma estrutura interpret√°vel do conjunto de textos.

Nesta etapa, a implementa√ß√£o do LDA dispon√≠vel no scikit-learn foi utilizada para identificar 9 t√≥picos presentes nos textos da se√ß√£o "Mercado". Para garantir um processamento robusto, o n√∫mero m√°ximo de itera√ß√µes foi definido como 100 (`max_iter`), permitindo uma converg√™ncia mais precisa dos t√≥picos, e um valor de `random_seed` foi aplicado para garantir reprodutibilidade nos resultados.

O corpus textual √© convertido em uma matriz num√©rica (`tfidf`), que serve de entrada para o algoritmo LDA. Ap√≥s o treinamento, cada documento recebe um t√≥pico predominante, identificado atrav√©s da fun√ß√£o `get_topic()`, que calcula a distribui√ß√£o de t√≥picos do documento e seleciona aquele de maior relev√¢ncia. Esse resultado √© armazenado na coluna `topic`, permitindo a segmenta√ß√£o dos textos conforme os temas gerados pelo modelo.


### Parte 08. Visualiza√ß√£o dos dados

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

Para interpretar os resultados da an√°lise de t√≥picos, foram utilizadas t√©cnicas de visualiza√ß√£o gr√°fica e de nuvens de palavras, permitindo explorar a distribui√ß√£o dos temas extra√≠dos do conjunto de not√≠cias.

- **8.1. Gr√°fico de N√∫mero de Documentos vs T√≥picos**

    O gr√°fico gerado exibe a distribui√ß√£o dos t√≥picos nas not√≠cias, mostrando quantos documentos foram classificados em cada tema identificado pelo modelo LDA. As barras horizontais representam a frequ√™ncia de documentos por t√≥pico, e a escala logar√≠tmica no eixo X ajuda a visualizar diferen√ßas significativas na quantidade de not√≠cias por categoria.

    Esse tipo de visualiza√ß√£o facilita a interpreta√ß√£o dos t√≥picos mais recorrentes e quais temas tiveram maior presen√ßa no conjunto de textos analisados.

- **8.2. Nuvem de Palavras por T√≥picos**

    A nuvem de palavras √© uma t√©cnica de visualiza√ß√£o que destaca os termos mais frequentes dentro de cada t√≥pico identificado pelo modelo LDA. Quanto maior a palavra na imagem, maior sua relev√¢ncia dentro daquele contexto espec√≠fico, facilitando a interpreta√ß√£o dos principais temas abordados nos documentos.

    Nesta etapa, as colunas `spacy_lemma` e `topic` s√£o utilizadas para gerar nuvens de palavras para cada um dos 9 t√≥picos identificados. A fun√ß√£o `plot_wordcloud_for_a_topic()` extrai os termos lematizados pertencentes a cada t√≥pico e os converte em uma visualiza√ß√£o gr√°fica utilizando a biblioteca `WordCloud`.

    Cada gr√°fico gerado representa visualmente os termos mais recorrentes no respectivo t√≥pico, auxiliando na an√°lise sem√¢ntica e na interpreta√ß√£o dos temas abordados nos textos.

- **8.3. Nuvem de Palavras por Entidades**

    J√° a nuvem de palavras por entidades √© uma t√©cnica de visualiza√ß√£o que destaca organiza√ß√µes mencionadas em cada t√≥pico identificado pelo modelo `LDA`. Essa abordagem facilita a an√°lise da frequ√™ncia e relev√¢ncia das entidades dentro dos textos, ajudando a compreender quais institui√ß√µes ou empresas s√£o mais citadas em cada contexto.

    Nesta etapa, as colunas `spacy_ner` e `topic` tamb√©m foram utilizadas para criar as nuvens de palavras por entidades nomeadas. 
    
    A fun√ß√£o `plot_wordcloud_entities_for_a_topic()` processa os documentos e extrai organiza√ß√µes mencionadas, substituindo espa√ßos em nomes compostos por "_" para melhor exibi√ß√£o na nuvem de palavras.

    Cada t√≥pico recebe uma nuvem de entidades nomeadas, permitindo uma visualiza√ß√£o intuitiva das institui√ß√µes mais recorrentes nos textos analisados.


## Conclus√µes Gerais

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

Este projeto demonstrou a import√¢ncia do Processamento de Linguagem Natural na extra√ß√£o de t√≥picos e an√°lise de grandes volumes de texto. A partir de t√©cnicas como TF-IDF, LDA e reconhecimento de entidades nomeadas, foi poss√≠vel estruturar e interpretar os dados textuais, identificando padr√µes relevantes nos documentos analisados.

A aplica√ß√£o de pr√©-processamento com NLTK e spaCy garantiu 
que os textos fossem preparados de forma a permitir que os t√≥picos fossem segmentados atrav√©s do modelo LDA. Al√©m disso, a utiliza√ß√£o de Bag of Words e vetoriza√ß√£o TF-IDF possibilitou uma representa√ß√£o quantitativa dos textos, facilitando sua an√°lise computacional.

O projeto tamb√©m demonstrou a diferen√ßa entre `lematiza√ß√£o` e `stemming` no tratamento dos termos. 

Enquanto a lematiza√ß√£o, realizada com spaCy, transforma as palavras em suas formas base considerando contexto gramatical e significado, o stemming, utilizado via NLTK, aplica um recorte mais simples dos sufixos das palavras sem levar em conta sua estrutura lingu√≠stica completa. Essa distin√ß√£o se reflete diretamente na qualidade da an√°lise sem√¢ntica, tornando a lematiza√ß√£o mais apropriada para tarefas que exigem maior precis√£o na interpreta√ß√£o dos textos.

Por fim, as visualiza√ß√µes gr√°ficas, como distribui√ß√£o dos t√≥picos, nuvem de palavras e entidades nomeadas, ajudaram a tornar os resultados mais compreens√≠veis e acess√≠veis. A estrutura√ß√£o dos dados e os m√©todos aplicados refor√ßaram o potencial das t√©cnicas de PLN na interpreta√ß√£o de grandes corpora textuais, trazendo insights para a an√°lise sem√¢ntica.


## Sobre mim

‚¨ÜÔ∏è <a href='#√≠ndice'>Voltar ao in√≠cio</a>

<div>
    <img src="./docs/assets/logo_mt_ds.png" alt="mt_logo" style='width: 80px; vertical-align: middle;' align='left'>
    <span style="">Mateus Teixeira</span>
</div>
Cientista de dados
<br>
P√≥s-graduando em Intelig√™ncia Artifcial pela INFNET
<br>
<br>
<a href="mailto:pessoal.mtr@gmail.com"
    target="_blank">
    <img 
            src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&labelColor=555&logoColor=white" 
            alt="E-mail" 
            height='25'/>
</a>
<a href="https://www.linkedin.com/in/mateusteixeira/" 
        target="_blank">
    <img 
            src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&labelColor=555&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAACXBIWXMAAAsTAAALEwEAmpwYAAABNklEQVR4nO3XSUoDQRgG0NrFTRyiuBA8lARvIujanROKnigICh5AVASHkDPECZ60yULFVDqamEqoB73rGj6q6q/uELIsmz6o4QgPeEUTx1gMEzL5Kz+7xnxIGQ7F7YWU6WybmNuQMrz0CfAcUobHPgHuQ8p0qk/MfpiAKnTTY/KXyVehApZw0q3/xT1whwMsfLyQZVNMCcNohxVsoNEt3U9o4RRbWE0yACrYLnFhtrGLmWQCYA4XBnOO5VQCnA04+c/tKikE+IudSQ/QLnWwEw5Q2EwhQFEy66h2n7XIH+B3jXEHKCZf6/EB2SzRvjXuAPXIuOsl2r+NO0A1Mu7sb8f+twCjGvuLHCAi9GEIfeQViMkrIG+hjlEdQEPoIx/imLwC8hbqyIe4ByWEUXWSTIAsy8JEeQd/X93eUJrQcwAAAABJRU5ErkJggg==" 
            alt="LinkedIn" 
             height='25'/>
</a>
<a href="https://www.instagram.com/omateusteixeira" 
        target="_blank">
    <img 
            src="https://img.shields.io/badge/Instagram-E4405F?style=for-the-badge&logo=instagram&labelColor=555&logoColor=white" 
            alt="Instagram" 
             height='25'/>
</a>